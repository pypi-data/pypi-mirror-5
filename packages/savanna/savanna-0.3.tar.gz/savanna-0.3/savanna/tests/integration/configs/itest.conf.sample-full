[COMMON]

# Username for OpenStack
OS_USERNAME = 'admin'
# Password for OpenStack
OS_PASSWORD = 'admin'
# Tenant name for OpenStack
OS_TENANT_NAME = 'admin'
# URL for OpenStack
OS_AUTH_URL = 'http://127.0.0.1:35357/v2.0/'

# Host for Savanna API
SAVANNA_HOST = '127.0.0.1'
# Port for Savanna API
SAVANNA_PORT = '8386'
# Api version for Savanna
SAVANNA_API_VERSION = 'v1.1'

# Flavor ID for image
FLAVOR_ID = '2'

# Cluster creation timeout (in minutes); minimal value is 1
CLUSTER_CREATION_TIMEOUT = 30

# Timeout for node process deployment on cluster nodes (in minutes);
# minimal value is 1
TELNET_TIMEOUT = 5

# Timeout for HDFS initialization (in minutes); minimal value is 1
HDFS_INITIALIZATION_TIMEOUT = 5

# Timeout for job creation (in minutes); minimal value is 1
JOB_LAUNCH_TIMEOUT = 5

# Name for cluster
CLUSTER_NAME = 'test-cluster'

# OpenStack key pair id your SSH public key which Savanna transfers
# to cluster nodes for access of users to virtual machines via SSH,
# using this key'
USER_KEYPAIR_ID = 'jsmit'

# Path to folder where is located id_rsa key which is used for
# remote command execution
PATH_TO_SSH_KEY = '/home/ubuntu/.ssh/id_rsa'

[VANILLA]

# Name of plugin
PLUGIN_NAME = 'vanilla'

# ID for image which is used for cluster creation
IMAGE_ID = '123456-qwerty'

# Username which is used for connecting to cluster nodes via SSH
NODE_USERNAME = 'ubuntu'

# Version of Hadoop
HADOOP_VERSION = '1.2.1'
# Username which is used for access to Hadoop services
HADOOP_USER = 'hadoop'
# Directory where are located Hadoop jar files
HADOOP_DIRECTORY = '/usr/share/hadoop'
# Directory where is located log info about completed jobs
HADOOP_LOG_DIRECTORY = '/mnt/log/hadoop/hadoop/userlogs'

HADOOP_PROCESSES_WITH_PORTS = jobtracker: 50030, namenode: 50070, tasktracker: 50060, datanode: 50075, secondarynamenode: 50090, oozie: 11000

PROCESS_NAMES = nn: namenode, tt: tasktracker, dn: datanode

SKIP_ALL_TESTS_FOR_PLUGIN = False
SKIP_CLUSTER_CONFIG_TEST = False
SKIP_EDP_TEST = False
SKIP_MAP_REDUCE_TEST = False
SKIP_SWIFT_TEST = False
SKIP_SCALING_TEST = False

[HDP]

# Name of plugin
PLUGIN_NAME = 'hdp'

# ID for image which is used for cluster creation
IMAGE_ID = '123456-qwerty'

# Username which is used for connecting to cluster nodes via SSH
NODE_USERNAME = 'root'

# Version of Hadoop
HADOOP_VERSION = '1.3.2'
# Username which is used for access to Hadoop services
HADOOP_USER = 'hdfs'
# Directory where are located Hadoop jar files
HADOOP_DIRECTORY = '/usr/lib/hadoop'
# Directory where is located log info about completed jobs
HADOOP_LOG_DIRECTORY = '/hadoop/mapred/userlogs'

HADOOP_PROCESSES_WITH_PORTS = JOBTRACKER: 50030, NAMENODE: 50070, TASKTRACKER: 50060, DATANODE: 50075, SECONDARY_NAMENODE: 50090

PROCESS_NAMES = nn: NAMENODE,tt: TASKTRACKER,dn: DATANODE

SKIP_ALL_TESTS_FOR_PLUGIN = False
SKIP_MAP_REDUCE_TEST = False
SKIP_SWIFT_TEST = False
SKIP_SCALING_TEST = False