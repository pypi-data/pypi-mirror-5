# -*- coding: utf-8 -*-
"""
This module contains the Mozaik analysis interface and implementation of various analysis algorithms

For more documentation refer to `mozaik.analysis`_
"""
import numpy
import scipy
import time
import quantities as qt
import mozaik.tools.units as munits
from mozaik.tools.mozaik_parametrized import colapse, colapse_to_dictionary, MozaikParametrized
from mozaik.analysis.data_structures import PerNeuronValue, \
                                        ConductanceSignalList, AnalogSignalList, PerNeuronPairValue, SingleValue
from mozaik.analysis.helper_functions import psth
from mozaik.core import ParametrizedObject
from parameters import ParameterSet
from mozaik.storage import queries
from neo.core.analogsignal import AnalogSignal
from mozaik.tools.circ_stat import circ_mean, circular_dist
from mozaik.tools.neo_object_operations import neo_mean, neo_sum
import mozaik

logger = mozaik.getMozaikLogger()

class Analysis(ParametrizedObject):
    """
    The analysis interface.

    Each mozaik analysis is a subclass of this class and is required to implementat
    the `perform_analysis` function which should contain the analysis code.
    This function should retrieve data from the `DataStoreView` that is
    supplied in the `datastore` parameter. Further, it should include `tags`
    as the tags for all `AnalysisDataStructure` objects that it creates.
    (See the description of the `analysis.AnalysisDataStructure.tags` attribute.

    Parameters
    ------------
    
    datastore : DataStoreView
                The datastore from which to pull data.
    parameters : ParameterSet
                 The required parameter set.
    tags : list(str), optional
           Tags to attach to the AnalysisDataStructures generated by the analysis.
    """

    def __init__(self, datastore, parameters, tags=None):
        ParametrizedObject.__init__(self, parameters)
        self.datastore = datastore
        if tags == None:
            self.tags = []
        else:
            self.tags = tags

    def analyse(self):
        """
        This is the function calls to perform the analysis.
        """
        t1 = time.time()
        logger.info('Starting ' + self.__class__.__name__ + ' analysis')
        self.perform_analysis()
        t2 = time.time()
        logger.warning(self.__class__.__name__ + ' analysis took: '
                       + str(t2-t1) + 'seconds')

    def perform_analysis(self):
        """
        This is an abstract function that each analysis class has to implement. It
        contains the code performing the analysis.
        """
        raise NotImplementedError


class TrialAveragedFiringRate(Analysis):
    """
    This analysis takes each recording in DSV that has been done in response to stimulus type 'stimulus_type' 
    and calculates the average (over trials) number of spikes. For each set of equal recordings (except trial) it creates one PerNeuronValue 
    `AnalysisDataStructure` instance containing the trial averaged firing rate per each recorded 
    neuron.
    """
    
    def perform_analysis(self):
        
        for sheet in self.datastore.sheets():
            dsv1 = queries.param_filter_query(self.datastore, sheet_name=sheet)
            segs = dsv1.get_segments()
            st = [MozaikParametrized.idd(s) for s in dsv1.get_stimuli()]
            # transform spike trains due to stimuly to mean_rates
            mean_rates = [numpy.array(s.mean_rates()) for s in segs]
            # collapse against all parameters other then trial
            (mean_rates, s) = colapse(mean_rates, st, parameter_list=['trial'])
            # take a sum of each
            mean_rates = [sum(a)/len(a) for a in mean_rates]

            #JAHACK make sure that mean_rates() return spikes per second
            units = munits.spike / qt.s
            logger.debug('Adding PerNeuronValue containing trial averaged firing rates to datastore')
            for mr, st in zip(mean_rates, s):
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(mr,segs[0].get_stored_spike_train_ids(),units,
                                   stimulus_id=str(st),
                                   value_name='Firing rate',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   analysis_algorithm=self.__class__.__name__,
                                   period=None))


class PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage(Analysis):
    """
    Calculates a preference and selectvitiy tuning of a periodic variable via vector average method.
    
    This analysis takes from the DSV all `PerNeuronValues`.
    All PerNeuronValues have to belong to stimuli of the same type and
    contain the same type of values (i.e. have the same `value_name`).

    For each combination of parameters of the stimuli other than `parameter_name`
    `PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage` creates a
    PerNeuronValue which corresponsd to the vector average through the
    periodic domain of `parameter_name`.
    
    Other parameters
    ---------------- 
    parameter_name : str
                   The name of the parameter through which to calculate the VectorAverage
    """

    required_parameters = ParameterSet({
        'parameter_name': str,  # The name of the parameter through which to calculate the VectorAverage
    })

    def perform_analysis(self):
        for sheet in self.datastore.sheets():
            # Get PerNeuronValue ASD and make sure they are all associated
            # with the same stimulus and do not differ in any
            # ASD parameters except the stimulus
            dsv = queries.param_filter_query(self.datastore, sheet_name=sheet,identifier='PerNeuronValue')
            if len(dsv.analysis_results) == 0:
                break
            assert queries.equal_ads_except(dsv, ['stimulus_id'])
            assert queries.ads_with_equal_stimulus_type(dsv, allow_None=True)
            assert len(set([a.value_name for a in dsv.get_analysis_result(sheet_name=sheet)])) , "PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage accepts only DSVs containing values with the same value_name"
            self.pnvs = dsv.get_analysis_result(sheet_name=sheet)
            # get stimuli
            st = [MozaikParametrized.idd(s.stimulus_id) for s in self.pnvs]
            
            # also make sure values are in the same order for each pnv (with respect to the first pnv id order)
            d = colapse_to_dictionary([z.get_value_by_id(self.pnvs[0].ids) for z in self.pnvs],
                                      st,
                                      self.parameters.parameter_name)
            for k in d.keys():
                keys, values = d[k]
                y = []
                x = []
                for v, p in zip(values, keys):
                    y.append(v)
                    x.append(numpy.zeros(numpy.shape(v)) + p)
                pref, sel = circ_mean(numpy.array(x),
                                      weights=numpy.array(y),
                                      axis=0,
                                      low=0,
                                      high=st[0].params()[self.parameters.parameter_name].period,
                                      normalize=True)

                logger.debug('PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage: Adding PerNeuronValue to datastore')
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(pref,
                                   self.pnvs[0].ids, 
                                   st[0].params()[self.parameters.parameter_name].units,
                                   value_name=self.parameters.parameter_name + ' preference',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   period=st[0].params()[self.parameters.parameter_name].period,
                                   analysis_algorithm=self.__class__.__name__,
                                   stimulus_id=str(k)))
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(sel,
                                   self.pnvs[0].ids,
                                   st[0].params()[self.parameters.parameter_name].units,
                                   value_name=self.parameters.parameter_name + ' selectivity',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   period=1.0,
                                   analysis_algorithm=self.__class__.__name__,
                                   stimulus_id=str(k)))


class GSTA(Analysis):
    """
    Computes conductance spike triggered average.
    
    Other parameters
    ---------------- 
    length : float (ms)
           how long before and after spike to compute the GSTA
           it will be rounded *down* to fit the sampling frequency 
           
    neurons : list
            the list of neuron ids for which to compute the GSTA
           
    Notes
    -----

    `GSTA` does not assume that spikes are aligned with the conductance
    sampling rate and will pick the bin in which the given spike falls
    (within the conductance sampling rate binning) as the center of the
    conductance vector that is included in the STA.
    """

    required_parameters = ParameterSet({
        'length': float,  # length (in ms time) how long before and after spike to compute the GSTA
                          # it will be rounded down to fit the sampling frequency
        'neurons': list,  # the list of neuron ids for which to compute the
    })

    def perform_analysis(self):
        dsv = self.datastore
        for sheet in dsv.sheets():
            dsv1 = queries.param_filter_query(dsv, sheet_name=sheet)
            st = dsv1.get_stimuli()
            segs = dsv1.get_segments()

            asl_e = []
            asl_i = []
            for n in self.parameters.neurons:
                sp = [s.get_spiketrain(n) for s in segs]
                g_e = [s.get_esyn(n) for s in segs]
                g_i = [s.get_isyn(n) for s in segs]
                asl_e.append(self._do_gsta(g_e, sp))
                asl_i.append(self._do_gsta(g_i, sp))
                self.datastore.full_datastore.add_analysis_result(
                ConductanceSignalList(asl_e,
                                      asl_i,
                                      self.parameters.neurons,
                                      sheet_name=sheet,
                                      tags=self.tags,
                                      analysis_algorithm=self.__class__.__name__))

    def _do_gsta(self, analog_signal, sp):
        dt = analog_signal[0].sampling_period
        gstal = int(self.parameters.length/dt)
        gsta = numpy.zeros(2*gstal + 1,)
        count = 0
        for (ans, spike) in zip(analog_signal, sp):
            for time in spike:
                if time > ans.t_start  and time < ans.t_stop:
                    idx = int((time - ans.t_start)/dt)
                    if idx - gstal > 0 and (idx + gstal + 1) <= len(ans):
                        gsta = gsta + ans[idx-gstal:idx+gstal+1].flatten().magnitude
                        count +=1
        if count == 0:
            count = 1
        gsta = gsta / count
        gsta = gsta * analog_signal[0].units

        return AnalogSignal(gsta,
                            t_start=-gstal*dt,
                            sampling_period=dt,
                            units=analog_signal[0].units)


class Precision(Analysis):
    """
    Computes the precision as the autocorrelation between the PSTH of
    different trials.

    Takes all the responses in the datastore, and for each recording group that is identical except the 
    trial number it will compute the autocorrelation between the PSTH of different trials. For each such 
    group it will create add a AnalogSignalList instance into the datastore containing the calculated autocorrelation
    vector for each neuron in `required_parameters.neurons`.
    
    Other parameters
    ---------------- 
    bin_length : float (ms)
               the size of bin to construct the PSTH from
    neurons: list
           the list of neuron ids for which to compute the autocorrelation
    """

    required_parameters = ParameterSet({
        'neurons': list,  # the list of neuron ids for which to compute the
        'bin_length': float,  # (ms) the size of bin to construct the PSTH from
    })

    def perform_analysis(self):
        for sheet in self.datastore.sheets():
            # Load up spike trains for the right sheet and the corresponding
            # stimuli, and transform spike trains into psth
            dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
            # make sure spiketrains are also order in the same way
            psths = [psth(seg.get_spiketrain(self.parameters.neurons), self.parameters.bin_length)
                     for seg in dsv.get_segments()]

            st = [MozaikParametrized.idd(s) for s in dsv.get_stimuli()]

            # average across trials
            psths, stids = colapse(psths,
                                   st,
                                   parameter_list=['trial'],
                                   func=neo_mean,
                                   allow_non_identical_objects=True)

            for ppsth, stid in zip(psths, stids):
                t_start = ppsth[0].t_start
                duration = ppsth[0].t_stop-ppsth[0].t_start
                al = []
                for idx in xrange(0,len(self.parameters.neurons)):
                    ac = numpy.correlate(numpy.array(ppsth[:, idx]),
                                         numpy.array(ppsth[:, idx]),
                                         mode='full')
                    div = numpy.sum(numpy.power(numpy.array(ppsth[:, idx]), 2))
                    if div != 0:
                        ac = ac / div
                    al.append(
                        AnalogSignal(ac,
                                     t_start=-duration+self.parameters.bin_length*t_start.units/2,
                                     sampling_period=self.parameters.bin_length*qt.ms,
                                     units=qt.dimensionless))

                logger.debug('Adding AnalogSignalList:' + str(sheet))
                self.datastore.full_datastore.add_analysis_result(
                    AnalogSignalList(al,
                                     self.parameters.neurons,
                                     qt.ms,
                                     qt.dimensionless,
                                     x_axis_name='time',
                                     y_axis_name='autocorrelation',
                                     sheet_name=sheet,
                                     tags=self.tags,
                                     analysis_algorithm=self.__class__.__name__,
                                     stimulus_id=str(stid)))



class TrialVariability(Analysis):
      """
      For each stimulus and neuron it calculates the trial-to-trial variability of Vm or conductance (depending on parameters). 
      It creates a AnalogSignalList list for each recording (except trials).
      
      Other parameters
      ---------------- 
      vm : bool
         calculate variability for Vm?
      
      cond_exc : bool
               calculate variability for excitatory conductance?
      
      cond_inh : bool
               calculate variability for inhibitory conductance?
         

      """
      required_parameters = ParameterSet({
        'vm': bool,  # calculate variability for Vm?
        'cond_exc': bool,  # calculate variability for excitatory conductance?
        'cond_inh': bool,  # calculate variability for inhibitory conductance?
      })
      
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                segs, stids = colapse(dsv.get_segments(),dsv.get_stimuli(),parameter_list=['trial'],allow_non_identical_objects=True)
                for segs,st in zip(segs,stids):
                    if self.parameters.vm:
                        first_vm = segs[0].get_vm(segs[0].get_stored_vm_ids()[0])
                        vm = [AnalogSignal(numpy.var(numpy.array([s.get_vm(i) for s in segs]),axis=0),t_start=first_vm.t_start,sampling_period=first_vm.sampling_period,units=first_vm.units) for i in segs[0].get_stored_vm_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(vm,segs[0].get_stored_vm_ids(),segs[0].get_vm(segs[0].get_stored_vm_ids()[0]).units,y_axis_name = 'vm trial-to-trial variance',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_exc:                        
                        first_cond = segs[0].get_esyn(segs[0].get_stored_esyn_ids()[0])
                        cond_exc = [AnalogSignal(numpy.var(numpy.array([s.get_esyn(i) for s in segs]),axis=0),t_start=first_cond.t_start,sampling_period=first_cond.sampling_period,units=first_cond.units) for i in segs[0].get_stored_esyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(cond_exc,segs[0].get_stored_esyn_ids(),segs[0].get_esyn(segs[0].get_stored_esyn_ids()[0]).units,y_axis_name = 'exc. conductance trial-to-trial variance',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_inh:                                    
                        first_cond = segs[0].get_isyn(segs[0].get_stored_isyn_ids()[0])            
                        cond_inh = [AnalogSignal(numpy.var(numpy.array([s.get_isyn(i) for s in segs]),axis=0),t_start=first_cond.t_start,sampling_period=first_cond.sampling_period,units=first_cond.units) for i in segs[0].get_stored_isyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(cond_inh,segs[0].get_stored_isyn_ids(),segs[0].get_isyn(segs[0].get_stored_isyn_ids()[0]).units,y_axis_name = 'inh. conductance trial-to-trial variance',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    

class GaussianTuningCurveFit(Analysis):
      """
      Fits each tuning curve with a gaussian.
      
      It takes a dsv containing some PerNeuronValues.
      All PerNeuronValues have to belong to stimuli of the same type and
      contain the same type of values (i.e. have the same `value_name`).

      For each combination of parameters of the stimuli other than `parameter_name`
      `GaussianTuningCurveFit` fits a gaussian tuning curve to the values in the PerNeuronValues
      that span the parameter values of the `parameter_name` stimulus parameter.
      
      It stores the parameters of the fitted Gaussian in a PerNeuronValue - for each combination of parameters of the stimulus associated 
      with the supplied PerNeuronValue with the exception of the stimulus parameter `parameter_name`. For each combination it stored 
      5 PerNeuronValue data structures, containing the calculated selectivity, preference, half-width at half-height (HWHH), maximum ('scale' of the gaussian) and 
      baseline (the constant offset).
    
      Other parameters
      ------------------- 
      parameter_name : str
                     The stimulus parameter name through which to fit the tuning curve.
    
      """   
      required_parameters = ParameterSet({
          'parameter_name': str  # the parameter_name through which to fit the tuning curve
      })      
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore,identifier='PerNeuronValue',sheet_name=sheet)
                if len(dsv.get_analysis_result()) == 0: continue
                assert queries.equal_ads_except(dsv, ['stimulus_id'])
                assert queries.ads_with_equal_stimulus_type(dsv)
                self.pnvs = dsv.get_analysis_result()
                
                # get stimuli
                self.st = [MozaikParametrized.idd(s.stimulus_id) for s in self.pnvs]
                
                # check whether it is periodic 
                period = self.st[0].params()[self.parameters.parameter_name].period
                
                # transform the pnvs into a dictionary of tuning curves according along the parameter_name
                # also make sure they are ordered according to the first pnv's idds 
                
                self.tc_dict = colapse_to_dictionary([z.get_value_by_id(self.pnvs[0].ids) for z in self.pnvs],self.st,self.parameters.parameter_name)
                for k in self.tc_dict.keys():
                        z = []
                        for i in xrange(0,len(self.pnvs[0].values)):
                            res = self._fitgaussian(self.tc_dict[k][0],[a[i] for a in self.tc_dict[k][1]],period)
                            if res == None:
                               logger.debug('Failed to fit tuning curve %s for neuron %d' % (k,i))
                               return
                            z.append(res)    
                        #import pylab
                        #pylab.show()
                        res = numpy.array(z)
                        if res != None:
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,0],self.pnvs[0].ids,self.pnvs[0].value_units,value_name = self.parameters.parameter_name + ' baseline',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,1],self.pnvs[0].ids,self.pnvs[0].value_units,value_name = self.parameters.parameter_name + ' max',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,2],self.pnvs[0].ids,MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' selectivity',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,3],self.pnvs[0].ids,MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' preference',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(180*res[:,2]/numpy.pi*2.35482,self.pnvs[0].ids,MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' HWHH',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                        else:
                           logger.debug('Failed to fit tuning curve %s for neuron %d' % (k,i))
                            
                    
       
      def _fitgaussian(self,X,Y,period):
          if period != None:
            fitfunc = lambda p,x:  p[0] + p[1]*numpy.exp(-circular_dist(p[3],x,period)**2/(2*p[2]**2))
          else:
            fitfunc = lambda p,x:  p[0] + p[1]*numpy.exp(-numpy.abs(p[3]-x)**2/(2*p[2]**2))  
          errfunc = lambda p, x, y: fitfunc(p,x) - y # Distance to the target function
          
          p0 = [0, 1.0, 0.5,0.0] # Initial guess for the parameters
          p0[0] = numpy.min(Y)
          if period != None:
            p0[3] = circ_mean(numpy.array([X]),weights=numpy.array([Y]),axis=1,low=0,high=period,normalize=True)[0]
          else:
            p0[3] = numpy.average(numpy.array(X),weights=numpy.array(Y))[0]
            
          p1, success = scipy.optimize.leastsq(errfunc, p0[:], args=(X,Y))      
          
          # if the fit is very bad - error greater than 30% of the Y magnitude
          if numpy.linalg.norm(fitfunc(p1,X)-Y)/numpy.linalg.norm(Y) > 0.1:
             p1 = [0,0,0,0]
                    
          if success:
            return p1
          else :
            return [0,0,0,0]

class PSTH(Analysis):
      """
      For each recording in the datastore view it creates an AnalogSignalList containing the PSTH of the neuron
      using the bin length `required_parameters.bin_length`.
      
      Other parameters
      ------------------- 
      bin_length : float
                 The bin length of the PSTH
    
      """  
      required_parameters = ParameterSet({
        'bin_length': float,  # the bin length of the PSTH
      })
      def perform_analysis(self):
            # make sure spiketrains are also order in the same way
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore,sheet_name=sheet)
                for st,seg in zip([MozaikParametrized.idd(s) for s in dsv.get_stimuli()],dsv.get_segments()):
                    psths = psth(seg.get_spiketrain(seg.get_stored_spike_train_ids()), self.parameters.bin_length)
                    self.datastore.full_datastore.add_analysis_result(
                        AnalogSignalList(psths,
                                         seg.get_stored_spike_train_ids(),
                                         psths[0].units,
                                         x_axis_name='time',
                                         y_axis_name='psth (bin=' + str(self.parameters.bin_length) + ')',
                                         sheet_name=sheet,
                                         tags=self.tags,
                                         analysis_algorithm=self.__class__.__name__,
                                         stimulus_id=str(st)))

class TemporalBinAverage(Analysis):
      """
      For each recording or AnalogSingalList ADS `with x_axis_name`='time' in the datastore `TemporalBinAverage` creates a new
      AnalogSingalList, containing a down-sampled version of the signal (vm, conductances or the AnalogSingalList), such that it will
      bin the time axis with bin length `required_parameters.bin_length` and make average for each bin.
      
      Other parameters
      ------------------- 
      bin_length : float
                 The bin length of the PSTH
      
      vm : bool
         calculate TemporalBinAverage for Vm?
      
      cond_exc : bool
               calculate TemporalBinAverage for excitatory conductance?
      
      cond_inh : bool
               calculate TemporalBinAverage for inhibitory conductance?
      """
      required_parameters = ParameterSet({
        'bin_length' : float,
        'vm': bool,  # calculate TemporalBinAverage for Vm?
        'cond_exc': bool,  # calculate TemporalBinAverage for excitatory conductance?
        'cond_inh': bool,  # calculate TemporalBinAverage for inhibitory conductance?
      })
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                
                # First let's do recordings
                for seg,st in zip(dsv.get_segments(),dsv.get_stimuli()):
                    if self.parameters.vm:
                        vm = [_down_sample_analog_signal_list(seg.get_vm(i))  for i in seg.get_stored_vm_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(vm,seg.get_stored_vm_ids(),vm[0].units,y_axis_name = 'bin averaged vm',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_exc:                        
                        e_syn = [_down_sample_analog_signal_list(seg.get_esyn(i))  for i in seg.get_stored_esyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(e_syn,seg.get_stored_esyn_ids(),e_syn[0].units,y_axis_name = 'bin averaged exc. cond.',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_inh:                                    
                        i_syn = [_down_sample_analog_signal_list(seg.get_isyn(i))  for i in seg.get_stored_isyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(AnalogSignalList(i_syn,seg.get_stored_isyn_ids(),i_syn[0].units,y_axis_name = 'bin averaged inh. cond.',x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                
                # Next let's do AnalogSingalLists
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet,identifier='AnalogSignalList')
                for asl in dsv.get_analysis_results():
                    assert isinstance(asd,'AnalogSignalList')
                    self.datastore.full_datastore.add_analysis_result(AnalogSignalList([down_sample_analog_signal_list(analog_signal,bin_length) for analog_signal in asl],asl.ids,asl.units,y_axis_name = 'bin averaged ' + asl.y_axis_name,x_axis_name="time",sheet_name=sheet,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=asl.stimulus_id))
                
      def _down_sample_analog_signal_list(analog_signal,bin_length):
          assert (analog_signal.t_stop.rescale(qt.ms) % bin_length)  < 0.00000001, "TemporalBinAverage: The analog signal length has to be divisible by bin_length"
          div = round(analog_signal.t_stop.rescale(qt.ms) / bin_length)
          return AnalogSignal(numpy.mean(numpy.reshape(analog_signal,(div,)),axis=1).flatten(),
                           t_start=0,
                           sampling_period=bin_length*qt.ms,
                           units=analog_signal.units)



class ActionPotentialRemoval(Analysis):
      """
      For each recording in the datastore view it creates an AnalogSignalList containing the VMs with 
      the spikes removed and sends it to the datastore. 
      
      Other parameters
      ------------------- 
      window_length : float (ms)
                    The length of the window starting at a spike time in which the Vm is replaced with linear interpolation.
      
      Notes
      -----
      Vm, in the window starting at each spike time, and lasting the window_length is removed and replaced by linear
      interpolation between the values of Vm at the extrems of the  cutout window.
      """  

      required_parameters = ParameterSet({
        'window_length': float,  # the length of window in which the Vm starting at a spike time is replaced with interpolation (in milliseconds)
      })
      
      @staticmethod
      def _remove_spikes(vm,spike_train,window_length):
          new_vm=vm[:]
          assert (window_length - int((window_length / vm.sampling_period.rescale(qt.ms).magnitude)) * vm.sampling_period.rescale(qt.ms).magnitude) < 0.00000000000001, ("%f" % (window_length % vm.sampling_period.rescale(qt.ms).magnitude))
          for spike_time in spike_train:
              spike_time_in_vm = int(spike_time / vm.sampling_period)
              assert spike_time_in_vm != len(vm)-1
              # we assume spike_time and sampling rates are in ms
              window_end = spike_time_in_vm+round(window_length/vm.sampling_period.rescale(qt.ms).magnitude)
              if window_end >= len(vm):
                 window_end = len(vm)-1
              if spike_time_in_vm != 0:
                new_vm[spike_time_in_vm:window_end] = vm[spike_time_in_vm-1] + (vm[window_end]-vm[spike_time_in_vm-1])*numpy.linspace(0,1.0,(window_end-spike_time_in_vm)+2)[1:-1]
              else:
                new_vm[spike_time_in_vm:window_end] = vm[spike_time_in_vm] + (vm[window_end]-vm[spike_time_in_vm])*numpy.linspace(0,1.0,(window_end-spike_time_in_vm)+2)[1:-1]
          return new_vm
            
      def perform_analysis(self):
            # make sure spiketrains are also order in the same way
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore,sheet_name=sheet)
                for st,seg in zip([MozaikParametrized.idd(s) for s in dsv.get_stimuli()],dsv.get_segments()):
                    asl = [ActionPotentialRemoval._remove_spikes(seg.get_vm(i),seg.get_spiketrain(i),self.parameters.window_length) for i in seg.get_stored_vm_ids()]
                    self.datastore.full_datastore.add_analysis_result(
                        AnalogSignalList(asl,
                                         seg.get_stored_vm_ids(),
                                         seg.get_vm(seg.get_stored_vm_ids()[0]).units,
                                         x_axis_name='time',
                                         y_axis_name='Vm (no AP)',
                                         sheet_name=sheet,
                                         tags=self.tags,
                                         analysis_algorithm=self.__class__.__name__,
                                         stimulus_id=str(st)))

      
                
class Irregularity(Analysis):
      """
      Irregularity as defined in:
      Kumar, A., Schrader, S., Aertsen, A., & Rotter, S. (2008). The high-conductance state of cortical networks. Neural computation, 20(1), 1-43. 
      It is the square of inter spike interval coefficient of variation.
      
      It creates on `PerNeuronValue` ADS per each recording in the DSV.
      
      Notes
      -----
      It is not possible to compute CV for neurons with no spikes. Therefore we exclude such neurons from the analysis.
      This means that the resulting PerNeuronValue ADS might not contain all ids of neurons as the source recording.
      """

      def perform_analysis(self):
          for sheet in self.datastore.sheets():
                # Load up spike trains for the right sheet and the corresponding stimuli, and
                # transform spike trains into psth
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                for seg,st in zip(dsv.get_segments(),dsv.get_stimuli()):
                    cv_isi=seg.cv_isi()
                    # Lets get rid of neurons for which ISI it was not possible to compute cv
                    to_delete = [i for i, x in enumerate(cv_isi) if x == None]
                    ids=numpy.delete(seg.get_stored_isyn_ids(),to_delete)
                    cv_isi=numpy.power(numpy.delete(cv_isi,to_delete),2)
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(cv_isi,ids,qt.dimensionless,value_name = 'CV of ISI squared',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))
           




class NeuronToNeuronAnalogSignalCorrelations(Analysis):
      """
      Calculates the pairwise correlation of AnalogSignal object for each pair of neurons.
      It creates one PerNeuronPairValue for each AnalogSignalList ADS present in the DSV.
      
      Parameters
      ----------
      convert_nan_to_zero : bool,  
                          If true nan values in correlation coefficients which can result from non-varying varialbes will be turned to zeros.
      """

      required_parameters = ParameterSet({
        'convert_nan_to_zero': bool,  # if true nan values in correlation coefficients which can result from non-varying varialbes will be turned to zeros.
      })

      def perform_analysis(self):
           for sheet in self.datastore.sheets():
                # Load up spike trains for the right sheet and the corresponding stimuli, and
                # transform spike trains into psth
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet,identifier='AnalogSignalList')
                for asl in dsv.get_analysis_result():
                    corrs = numpy.nan_to_num(numpy.corrcoef(asl.asl))
                    self.datastore.full_datastore.add_analysis_result(PerNeuronPairValue(corrs,asl.ids,qt.dimensionless,value_name = 'Correlation coefficient(' +asl.y_axis_name + ')',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=asl.stimulus_id))        



class PopulationMean(Analysis):
      """
      Calculates the mean value accross population of a quantity. Currently it can process PerNeuronValues and PerNeuronPairValue ADS.
      This list might be increased.
      """
      def perform_analysis(self):
          dsv = queries.param_filter_query(self.datastore,identifier=['PerNeuronPairValue','PerNeuronValue'])
          for ads in dsv.get_analysis_result():
              if ads.period == None:
                 m = numpy.mean(ads.values)
              else:
                 m = circ_mean(ads.values.flatten(),high=ads.period) 
              self.datastore.full_datastore.add_analysis_result(SingleValue(value=m,period=ads.period,value_name = 'Mean(' +ads.value_name + ')',sheet_name=ads.sheet_name,tags=self.tags,analysis_algorithm=self.__class__.__name__,stimulus_id=ads.stimulus_id))        
              
