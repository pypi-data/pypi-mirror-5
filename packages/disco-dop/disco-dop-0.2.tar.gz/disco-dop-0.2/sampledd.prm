stages=[
dict(
    name='pcfg',
    mode='pcfg',
    split=True,
    markorigin=True, #when splitting nodes, mark origin: VP_2 => {VP*1, VP*2}
    getestimates=None, #compute & store estimates
    useestimates=None, #load & use estimates
),
dict(
    name='plcfrs',
    mode='plcfrs',
    prune=True, #whether to use previous chart to prune parsing of this stage
    splitprune=True, #VP_2[101] is treated as { VP*[100], VP*[001] } during parsing
    k = 10000, #number of coarse pcfg derivations to prune with; k=0 => filter only
    neverblockre=None, #do not prune nodes with label that match regex
    getestimates=None, #compute & store estimates
    useestimates=None, #load & use estimates
),
dict(
   name='dop',
   mode='plcfrs',
   markorigin=True, #when splitting nodes, mark origin: VP_2 => {VP*1, VP*2}
   prune=True,    #whether to use previous chart to prune parsing of this stage
   k = 50,     #number of coarse plcfrs derivations to prune with; k=0 => filter only
   dop=True,
   usedoubledop=True,
   newdd=False, #use experimental, more efficient double dop algorithm
   iterate=False, #for double dop, whether to include fragments of fragments
   complement=False, #for double dop, whether to include fragments which form
           #the complement of the maximal recurring fragments extracted
   sample=False, kbest=True,
   m = 10000,      #number of derivations to sample/enumerate
   estimator="dop1", # choices: dop1, ewe, shortest, sl-dop[-simple]
   sldop_n=7,
   neverblockre=None, #do not prune nodes with label that match regex
   getestimates=None, #compute & store estimates
   useestimates=None, #load & use estimates
)
],
corpusdir=".",
#traincorpus="sample2.export", trainencoding="iso-8859-1", trainsents=3,
#testcorpus="sample2.export", testencoding="iso-8859-1", testsents=3,
traincorpus="negraproc.export", trainencoding="utf-8", trainsents=2000,
testcorpus="negraproc.export", testencoding="utf-8", testsents=200,
skip=0, # skip (additional) sentences between train & test set
#skiptrain=False, # when the train & test set are read from the same file,
#		# enable this to skip the training sentences to get to the test set.
skiptrain=True,
testmaxwords=40,  # max number of words for sentences in test corpus
trainmaxwords=40, # max number of words for sentences in train corpus
movepunct=True, # re-attach punctuation to appropriate constituents
removepunct=False, # remove all punctuation
unfolded=False, # apply transformations for Negra/Tiger to make trees less flat
usetagger=None,	#default is to use gold tags from treebank.
headrules="negra.headrules",
bintype="binarize", # choices: binarize, nltk, optimal, optimalhead
factor="right",
revmarkov=True,
v=1,
h=1,
leftMostUnary=True,
rightMostUnary=True,
fanout_marks_before_bin=False,
tailmarker="",
quiet=False, reallyquiet=False, #quiet=no per sentence results
numproc=1,  #increase to use multiple CPUs. Set to None to use all CPUs.
