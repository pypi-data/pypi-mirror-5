#!/bin/bash
#PBS -N GMX_MD
#PBS -j oe
#PBS -l select=8:ncpus=8:mpiprocs=8
#PBS -l walltime=00:20:00

# host: jade.cines.fr
# queuing system: PBS

# array jobs: 
#    -J N-M:S --> PBS_ARRAY_INDEX

# chain jobs with 
#    NEWID=$(qsub -N GMXMD_00  jade.pbs)
#    NEWID=$(qsub -N GMXMD_01 -W depend=afterok:$NEWID jade.pbs)
#    NEWID=$(qsub -N GMXMD_02 -W depend=afterok:$NEWID jade.pbs)

# set this to the same value as walltime; mdrun will stop cleanly
# at 0.99 * WALL_HOURS 
WALL_HOURS=0.33

# deffnm line is possibly modified by gromacs.setup
# (leave it as it is in the template)
DEFFNM=md

TPR=${DEFFNM}.tpr
OUTPUT=${DEFFNM}.out
PDB=${DEFFNM}.pdb

MDRUN_OPTS=""

# use this for, e.g. optimum number of pme nodes (-npme N)
# NOTE: This is system dependent; you MUST benchmark your system!!
# 24 pme on 64 cores best for 65k atom system
##MDRUN_OPTIONS="-npme 24"
# 16 pme on 64 cores best for 80k atom system
MDRUN_OPTS="$MDRUN_OPTS -npme 16"

# JOB_ARRAY_PLACEHOLDER

echo "--- PBS_NODEFILE"
cat $PBS_NODEFILE
cd $PBS_O_WORKDIR

# avoids some failures
export MPI_GROUP_MAX=1024

#module load gromacs

# use hard coded path for time being
GMXBIN="/opt/software/SGI/gromacs/4.0.3/bin"

MPIRUN=/usr/pbs/bin/mpiexec
APPLICATION=$GMXBIN/mdrun_mpi


$MPIRUN $APPLICATION -step 1000 -deffnm ${DEFFNM} -s ${TPR} -c ${PDB} -cpi \
                     $MDRUN_OPTS \
                     -maxh ${WALL_HOURS} > $OUTPUT
rc=$?

# dependent jobs will only start if rc == 0
exit $rc
